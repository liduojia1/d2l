# DRNN

![Untitled](DRNN%20d80dc8f6dc5f4c82b3f2ef51d6f89124/Untitled.jpeg)

$\mathbf{H}_t^{(l)} = \phi_l(\mathbf{H}_t^{(l-1)} \mathbf{W}_{xh}^{(l)} + \mathbf{H}_{t-1}^{(l)} \mathbf{W}_{hh}^{(l)}  + \mathbf{b}_h^{(l)}),\\\mathbf{O}_t = \mathbf{H}_t^{(L)} \mathbf{W}_{hq} + \mathbf{b}_q$

$\mathbf{X}_t \in \mathbb{R}^{n \times d},\mathbf{H}_t^{(l)} \in \mathbb{R}^{n \times h},\mathbf{O}_t \in \mathbb{R}^{n \times q},\mathbf{W}_{xh}^{(l)} \in \mathbb{R}^{h \times h},\mathbf{W}_{hh}^{(l)} \in \mathbb{R}^{h \times h},\mathbf{b}_h^{(l)} \in \mathbb{R}^{1 \times h}$

$\mathbf{W}_{hq} \in \mathbb{R}^{h \times q},\mathbf{b}_q \in \mathbb{R}^{1 \times q}$

## 实现

与lstm API实现唯一的区别就是设定了num_layers

```python
batch_size, num_steps = 32, 35
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)
vocab_size, num_hiddens, num_layers = len(vocab), 256, 2
num_inputs = vocab_size
device = d2l.try_gpu()
```

```python
lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers)
model = d2l.RNNModel(lstm_layer, len(vocab))
model = model.to(device)
num_epochs, lr = 500, 2
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)
```

![Untitled](DRNN%20d80dc8f6dc5f4c82b3f2ef51d6f89124/Untitled.png)