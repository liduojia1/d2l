# ResNet

## 残差块

在残差块中，输入可通过跨层数据线路更快地向前传播。                            

![左图虚线框中的部分需要直接拟合出该映射f(x)，而右图虚线框中的部分则需要拟合出残差映射f(x)−x。](ResNet%20c8d6867c4ad24f6eb5b48c203e533ed4/Untitled.png)

左图虚线框中的部分需要直接拟合出该映射f(x)，而右图虚线框中的部分则需要拟合出残差映射f(x)−x。

## ResNet的残差块

![                                           包含 以及 不包含 1×1卷积层 的残差块。](ResNet%20c8d6867c4ad24f6eb5b48c203e533ed4/Untitled%201.png)

                                           包含 以及 不包含 1×1卷积层 的残差块。

## ResNet架构

![         ResNet-18架构](ResNet%20c8d6867c4ad24f6eb5b48c203e533ed4/Untitled%202.png)

         ResNet-18架构

## 残差块代码实现

- 残差块的设计实现

ResNet沿用了VGG完整的3×3卷积层设计。 残差块里首先**有2个有相同输出通道数的3×3卷积层**。 每个卷积层后接一个**批量规范化层**和**ReLU激活函数**。 然后我们通过跨层数据通路，跳过这2个卷积运算，**将输入直接加在最后的ReLU激活函数前**。 这样的设计**要求2个卷积层的输出与输入形状一样**，从而使它们可以相加。 如果想改变通道数，就需要引入一个额外的1×1卷积层来将输入变换成需要的形状后再做相加运算。 残差块的实现如下：

```python
class Residual(nn.Module):  #@save
    def __init__(self, input_channels, num_channels,
                 use_1x1conv=False, strides=1):
        super().__init__()
        self.conv1 = nn.Conv2d(input_channels, num_channels,
                               kernel_size=3, padding=1, stride=strides)
        self.conv2 = nn.Conv2d(num_channels, num_channels,
                               kernel_size=3, padding=1)
        if use_1x1conv:
            self.conv3 = nn.Conv2d(input_channels, num_channels,
                                   kernel_size=1, stride=strides)
        else:
            self.conv3 = None
        self.bn1 = nn.BatchNorm2d(num_channels)
        self.bn2 = nn.BatchNorm2d(num_channels)

    def forward(self, X):
        Y = F.relu(self.bn1(self.conv1(X)))
        Y = self.bn2(self.conv2(Y))
        if self.conv3:
            X = self.conv3(X)
        Y += X
        return F.relu(Y)
```

```python
#下面我们来查看输入和输出形状一致的情况。
blk = Residual(3,3)
# X的前两个参数是输入输出通道数     后两个是输出的size
X = torch.rand(4, 3, 6, 6)
Y = blk(X)
Y.shape
```

![由于输入的残差块的input_channels和num_channels都是3，也就是应用3*3的卷积块，所以这个地方不需要用1*1卷积（当然也可以用）                                                                                                                                                     同时X的输入输出通道数 4 * 3经过3*3卷积，结果还是4*3（relu不会改变size）（这里暂时不考虑padding和stride改变输出的大小）](ResNet%20c8d6867c4ad24f6eb5b48c203e533ed4/Untitled%203.png)

由于输入的残差块的input_channels和num_channels都是3，也就是应用3*3的卷积块，所以这个地方不需要用1*1卷积（当然也可以用）                                                                                                                                                     同时X的输入输出通道数 4 * 3经过3*3卷积，结果还是4*3（relu不会改变size）（这里暂时不考虑padding和stride改变输出的大小）

- **X的第二个参数（output channel）和blk的第一个参数（input channel）的维度要保持一直，至于残差块使用的卷积核的输出输出大小不同时可以通过 1*1 卷积核调整**
- **最后Y的输出的channels的size即为X的第一个参数（input channel）和blk的第二个参数（output channel）**
- **与此同时，输出Y的后两个参数的shape此处是由stride和X的size共同决定**
- **代码里使用的卷积核均为3*3的size，如果stride改变，应该把use1*1conv设置成True**

```python
#我们也可以在增加输出通道数的同时，减半输出的高和宽。
blk = Residual(3,6, use_1x1conv=True, strides=2)
blk(X).shape
```

![Untitled](ResNet%20c8d6867c4ad24f6eb5b48c203e533ed4/Untitled%204.png)

- demo Residual 输入输出维度参考

![Untitled](ResNet%20c8d6867c4ad24f6eb5b48c203e533ed4/Untitled%205.png)

## ResNet模型

- ResNet的前两层（卷积 + 池化）
    
    在输出通道数为64、步幅为2的7×7卷积层后，增加一个批量规范化层，随后接步幅为2的3×3的最大汇聚层。 
    

```python
b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),
                   nn.BatchNorm2d(64), nn.ReLU(),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
```

- 随后ResNet使用4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。 第一个模块的通道数同输入通道数一致。 由于之前已经使用了步幅为2的最大汇聚层，所以无须减小高和宽。 之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。

```python
def resnet_block(input_channels, num_channels, num_residuals,
                 first_block=False):
    blk = []
    for i in range(num_residuals):
        if i == 0 and not first_block:
            blk.append(Residual(input_channels, num_channels,
                                use_1x1conv=True, strides=2))
        else:
            blk.append(Residual(num_channels, num_channels))
    return blk
```

- 接着在ResNet加入所有残差块，这里每个模块使用2个残差块。

```python
b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))
b3 = nn.Sequential(*resnet_block(64, 128, 2))
b4 = nn.Sequential(*resnet_block(128, 256, 2))
b5 = nn.Sequential(*resnet_block(256, 512, 2))
```

- ResNet最后加入全局平均汇聚层，以及全连接层输出。

```python
net = nn.Sequential(b1, b2, b3, b4, b5,
                    nn.AdaptiveAvgPool2d((1,1)),
                    nn.Flatten(), nn.Linear(512, 10))
```

测试demo：

```python
X = torch.rand(size=(1, 1, 224, 224))
for layer in net:
    X = layer(X)
    print(layer.__class__.__name__,'output shape:\t', X.shape)
```

![Step1：输入数据的维度为 （`1 * 1 * 224 * 224`） （前两个为输入输出通道数，后两个为size）                                                            Step2：经过b1残差块：通道数改变为1*64，同时size也经过两次改变，首先是`kernel_size=7, stride=2, padding=3`的卷积，将shape变为112；随后经过`kernel_size=3, stride=2, padding=1` 的maxpool，将shape变为56，所以此时的size为（`1 * 64 * 56 *56`）                                                                                                                                                Step3：经过b2残差块，b2不使用1 * 1卷积，也不设置stride步幅，所以size不改变，仍为56；同时将输入输出的channel均设为64，仍保持不变，所以此时的size为（`1 * 64 * 56 *56`）                                                                                                                                                                     Step4：经过b3残差块，使用1 * 1卷积，stirde = 2，size减半为28；同时将输入输出的channel设为（64，128），所以此时的size为（`1 * 128 * 28 *28`）                                                                                                                                                                                          Step5，6：经过b4 b5残差块，原理同Step4的b2残差块（`1 * 256 * 14 * 14`），（`1 * 512 * 7 *7`）                                                                                                              Step7：加入全局平均汇聚层，将size变为1 * 1，最后结果为（`1, 512, 1, 1`）                                                                                  Step8：展平与全连接层输出 `nn.Linear(512, 10))` 最后结果为（`1, 10` ）](ResNet%20c8d6867c4ad24f6eb5b48c203e533ed4/Untitled%206.png)

Step1：输入数据的维度为 （`1 * 1 * 224 * 224`） （前两个为输入输出通道数，后两个为size）                                                            Step2：经过b1残差块：通道数改变为1*64，同时size也经过两次改变，首先是`kernel_size=7, stride=2, padding=3`的卷积，将shape变为112；随后经过`kernel_size=3, stride=2, padding=1` 的maxpool，将shape变为56，所以此时的size为（`1 * 64 * 56 *56`）                                                                                                                                                Step3：经过b2残差块，b2不使用1 * 1卷积，也不设置stride步幅，所以size不改变，仍为56；同时将输入输出的channel均设为64，仍保持不变，所以此时的size为（`1 * 64 * 56 *56`）                                                                                                                                                                     Step4：经过b3残差块，使用1 * 1卷积，stirde = 2，size减半为28；同时将输入输出的channel设为（64，128），所以此时的size为（`1 * 128 * 28 *28`）                                                                                                                                                                                          Step5，6：经过b4 b5残差块，原理同Step4的b2残差块（`1 * 256 * 14 * 14`），（`1 * 512 * 7 *7`）                                                                                                              Step7：加入全局平均汇聚层，将size变为1 * 1，最后结果为（`1, 512, 1, 1`）                                                                                  Step8：展平与全连接层输出 `nn.Linear(512, 10))` 最后结果为（`1, 10` ）