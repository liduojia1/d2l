# 研究汇报

## 论文

- [ ]  BLUE socre：
    
    读论文了解了bluescore的机制即计算公式，并在nltk里面调用计算了一下。
    
- [ ]  Adam优化器：
    
    读论文了解了Adam优化器以及AdamGrad的基本原理，RMSprop，SGD的原理也复习了一下，这方面没有自己动手从头复现Adam优化器。
    
- [ ]  Attention：
    
    读了attention is all you need，注意力机制，transformer架构都基本知道。
    
- [ ]  束搜索

## Coding

由于之前对RNN没有过多地接触过，读了几篇论文以后感觉纸上谈兵不太行，想要复现论文首先得熟悉一下RNN相关的基础操作。

- [ ]  编程实现了比较基础的循环神经网络（RNN）
    
    跟着**文本预测**的教程手动实现了一遍数据预处理、语言模型、model、train、predict等操作。
    
- [ ]  GRU，LSTM手动实现了一遍。（不是调用nn.GRU, nn.LSTM）

## 计划：

- [ ]  读一下seq to seq这篇论文（包块encoder-decoder相关操作），自己根据已有数据集实现一下简单的机器翻译。
- [ ]  编码实现注意力机制，参考开源代码复现transformer
- [ ]  有时间再看一下BERT，之前读过BERT，但是没有太看明白